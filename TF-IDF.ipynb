{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Imports"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c60fed3cefab9b02"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "import re\n",
    "import pandas as pd\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:46:46.775064300Z",
     "start_time": "2024-02-29T14:46:46.769414Z"
    }
   },
   "id": "31a9395a76f99a29",
   "execution_count": 60
  },
  {
   "cell_type": "markdown",
   "source": [
    "Database:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b388cf46458f188"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "example_text = \"Cats are loved by everyone\"\n",
    "\n",
    "example_text2 = \"I love cats so much\"\n",
    "\n",
    "example_text3 = \"OMG cats are amazing, I love them\"\n",
    "\n",
    "database = [\n",
    "    example_text,\n",
    "    example_text2,\n",
    "    example_text3\n",
    "]\n",
    "\n",
    "example_text4 = \"Cats are amazing creatures, enchanting us with their graceful movements and independent personalities. Their soft purrs and gentle nuzzles provide comfort, fostering a unique bond with their human companions. With keen senses, agile leaps, and adorable antics, cats bring joy to homes worldwide. Their mysterious charm captivates hearts, making them truly extraordinary.\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:46:49.207514900Z",
     "start_time": "2024-02-29T14:46:49.206507Z"
    }
   },
   "id": "3164d2647f0a6aa2",
   "execution_count": 61
  },
  {
   "cell_type": "markdown",
   "source": [
    "TF-IDF\n",
    "\n",
    "There are multiple ways to calculate the TF(term frequency)\n",
    "\n",
    "I chose to use the Logarithmically scaled frequency\n",
    "\n",
    "IDF - inverse document frequency\n",
    "- looks ar common/uncommon words - corrects for words like as, of, the etc\n",
    "- minimize the weighting of frequent terms while making infrequent terms have a higher impact.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "63b8e9b038e15ab0"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: cats, LTFIDF: -0.6037\n",
      "Term: are, LTFIDF: 0.0000\n",
      "Term: loved, LTFIDF: 0.4055\n",
      "Term: by, LTFIDF: 0.4055\n",
      "Term: everyone, LTFIDF: 0.4055\n",
      "Term: i, LTFIDF: 0.0000\n",
      "Term: love, LTFIDF: 0.0000\n",
      "Term: so, LTFIDF: 0.4055\n",
      "Term: much, LTFIDF: 0.4055\n",
      "Term: omg, LTFIDF: 0.4055\n",
      "Term: amazing, LTFIDF: 0.4055\n",
      "Term: them, LTFIDF: 0.4055\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "def log_scaled_tf(raw_tf):\n",
    "    # Adding 1 to avoid log(0) issue\n",
    "    return 1 + math.log(raw_tf) if raw_tf > 0 else 0\n",
    "\n",
    "def ltfidf(database):\n",
    "    all_words = [re.findall(r'\\b\\w+\\b', text.lower()) for text in database]\n",
    "    \n",
    "    words = [word for sublist in all_words for word in sublist]\n",
    "\n",
    "    term_counts = Counter(words)\n",
    "    \n",
    "    total_terms = len(words)\n",
    "    \n",
    "    ltf = {term: log_scaled_tf(count) for term, count in term_counts.items()}\n",
    "    \n",
    "    term_document_count = Counter(term for doc in database for term in set(re.findall(r'\\b\\w+\\b', doc.lower())))\n",
    "    idf = {term: math.log(len(database) / (1 + term_document_count[term])) for term in ltf}\n",
    "\n",
    "    ltf_idf = {term: ltf[term] * idf[term] for term in ltf}\n",
    "\n",
    "    return ltf_idf\n",
    "\n",
    "ltfidf_result = ltfidf(database)\n",
    "\n",
    "for term, ltf_idf in ltfidf_result.items():\n",
    "    print(f\"Term: {term}, LTFIDF: {ltf_idf:.4f}\")\n",
    "\n",
    "print(\"==========\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:46:52.700165Z",
     "start_time": "2024-02-29T14:46:52.695150800Z"
    }
   },
   "id": "b9773be62dc8d3f5",
   "execution_count": 62
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then I decided that the simple TF should also work"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bd5cec3da6f43264"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: cats, TFIDF: -0.0508\n",
      "Term: are, TFIDF: 0.0000\n",
      "Term: loved, TFIDF: 0.0239\n",
      "Term: by, TFIDF: 0.0239\n",
      "Term: everyone, TFIDF: 0.0239\n",
      "Term: i, TFIDF: 0.0000\n",
      "Term: love, TFIDF: 0.0000\n",
      "Term: so, TFIDF: 0.0239\n",
      "Term: much, TFIDF: 0.0239\n",
      "Term: omg, TFIDF: 0.0239\n",
      "Term: amazing, TFIDF: 0.0239\n",
      "Term: them, TFIDF: 0.0239\n"
     ]
    }
   ],
   "source": [
    "def tfidf(database):\n",
    "    all_words = [re.findall(r'\\b\\w+\\b', text.lower()) for text in database]\n",
    "    \n",
    "    words = [word for sublist in all_words for word in sublist]\n",
    "\n",
    "    term_counts = Counter(words)\n",
    "    \n",
    "    total_terms = len(words)\n",
    "    \n",
    "    #so it's basically how frequent the term is/sum of all terms in the document (example text)\n",
    "\n",
    "    tf = {term: count / total_terms for term, count in term_counts.items()}\n",
    "        \n",
    "    #counts each unique term across all documents in database\n",
    "    term_document_count = Counter(term for examples in database for term in set(re.findall(r'\\b\\w+\\b', examples.lower())))\n",
    "    \n",
    "    #take the log of the number of documents over the set of documents that have the term term\n",
    "    idf = {term: math.log(len(database) / (1 + term_document_count[term])) for term in tf}\n",
    "\n",
    "    tf_idf = {term: tf[term] * idf[term] for term in tf}\n",
    "\n",
    "    return tf_idf\n",
    "\n",
    "tfidf_result = tfidf(database)\n",
    "\n",
    "for term, tf_idf in tfidf_result.items():\n",
    "    print(f\"Term: {term}, TFIDF: {tf_idf:.4f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:46:54.749286100Z",
     "start_time": "2024-02-29T14:46:54.746277700Z"
    }
   },
   "id": "54cdf7514ea9ba86",
   "execution_count": 63
  },
  {
   "cell_type": "markdown",
   "source": [
    "And then I discovered that you can do all these using TfidfVectorizer that basically doesn everything you need but in simpler t"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3bddcdcd7692204b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:46:59.511946100Z",
     "start_time": "2024-02-29T14:46:59.510389200Z"
    }
   },
   "id": "225414aa66dfc7eb",
   "execution_count": 64
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "idf values:\n",
      "amazing : 1.6931471805599454\n",
      "are : 1.2876820724517808\n",
      "by : 1.6931471805599454\n",
      "cats : 1.0\n",
      "everyone : 1.6931471805599454\n",
      "love : 1.2876820724517808\n",
      "loved : 1.6931471805599454\n",
      "much : 1.6931471805599454\n",
      "omg : 1.6931471805599454\n",
      "so : 1.6931471805599454\n",
      "them : 1.6931471805599454\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "result = tfidf.fit_transform(database)\n",
    "\n",
    "print('\\nidf values:')\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "for feature, idf_value in zip(feature_names, tfidf.idf_):\n",
    "    print(feature, ':', idf_value)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:47:00.696385300Z",
     "start_time": "2024-02-29T14:47:00.688374300Z"
    }
   },
   "id": "e6c40a1f3872ab67",
   "execution_count": 65
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word indexes:\n",
      "[('amazing', 0), ('are', 1), ('by', 2), ('cats', 3), ('everyone', 4), ('love', 5), ('loved', 6), ('much', 7), ('omg', 8), ('so', 9), ('them', 10)]\n"
     ]
    }
   ],
   "source": [
    "print('\\nWord indexes:')\n",
    "sorted_tfidf_vocabulary = sorted(tfidf.vocabulary_.items(), key=lambda x: x[1])\n",
    "print(sorted_tfidf_vocabulary)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:47:04.985642400Z",
     "start_time": "2024-02-29T14:47:04.983129300Z"
    }
   },
   "id": "52af77c3e73c8b3e",
   "execution_count": 66
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf-idf value:\n",
      "  (0, 4)\t0.5046113401371842\n",
      "  (0, 2)\t0.5046113401371842\n",
      "  (0, 6)\t0.5046113401371842\n",
      "  (0, 1)\t0.3837699307603192\n",
      "  (0, 3)\t0.2980315863446099\n",
      "  (1, 7)\t0.5844829010200651\n",
      "  (1, 9)\t0.5844829010200651\n",
      "  (1, 5)\t0.444514311537431\n",
      "  (1, 3)\t0.34520501686496574\n",
      "  (2, 10)\t0.4711101009983051\n",
      "  (2, 0)\t0.4711101009983051\n",
      "  (2, 8)\t0.4711101009983051\n",
      "  (2, 5)\t0.35829137488557944\n",
      "  (2, 1)\t0.35829137488557944\n",
      "  (2, 3)\t0.2782452148327134\n"
     ]
    }
   ],
   "source": [
    "print('\\ntf-idf value:')\n",
    "print(result)\n",
    "\n",
    "#(a, b) = a is document index, b is word index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:47:06.730082500Z",
     "start_time": "2024-02-29T14:47:06.726835500Z"
    }
   },
   "id": "c2f9dda45c3b1fb4",
   "execution_count": 67
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf-idf values with word indices:\n",
      "  (1, everyone:4) 0.5046\n",
      "  (1, by:2) 0.5046\n",
      "  (1, loved:6) 0.5046\n",
      "  (1, are:1) 0.3838\n",
      "  (1, cats:3) 0.2980\n",
      "  (2, much:7) 0.5845\n",
      "  (2, so:9) 0.5845\n",
      "  (2, love:5) 0.4445\n",
      "  (2, cats:3) 0.3452\n",
      "  (3, them:10) 0.4711\n",
      "  (3, amazing:0) 0.4711\n",
      "  (3, omg:8) 0.4711\n",
      "  (3, love:5) 0.3583\n",
      "  (3, are:1) 0.3583\n",
      "  (3, cats:3) 0.2782\n"
     ]
    }
   ],
   "source": [
    "print('\\ntf-idf values with word indices:')\n",
    "feature_names = tfidf.get_feature_names_out()\n",
    "\n",
    "for i, doc in enumerate(result):\n",
    "    for j, value in zip(doc.indices, doc.data):\n",
    "        word = feature_names[j]\n",
    "        print(f\"  ({i + 1}, {word}:{j}) {value:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:47:08.579347400Z",
     "start_time": "2024-02-29T14:47:08.571728Z"
    }
   },
   "id": "5c5ab364486e4562",
   "execution_count": 68
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tf-idf values in matrix form:\n",
      "[[0.         0.38376993 0.50461134 0.29803159 0.50461134 0.\n",
      "  0.50461134 0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.34520502 0.         0.44451431\n",
      "  0.         0.5844829  0.         0.5844829  0.        ]\n",
      " [0.4711101  0.35829137 0.         0.27824521 0.         0.35829137\n",
      "  0.         0.         0.4711101  0.         0.4711101 ]]\n"
     ]
    }
   ],
   "source": [
    "print('\\ntf-idf values in matrix form:')\n",
    "print(result.toarray())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-29T14:49:44.837546300Z",
     "start_time": "2024-02-29T14:49:44.836024900Z"
    }
   },
   "id": "5a2cee2608fd2e7",
   "execution_count": 70
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "e8a0e79f5d5fdc7d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "So basically, each row is a document(example text), and each column is a word from the corpus - we have 11 words for now so wooohoooo"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "608f2bfc0febeaef"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3178a0b2bd15b890"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
