{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:39:36.799678Z",
     "start_time": "2024-03-23T09:39:36.796488Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, RandomSampler, DataLoader\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:13:14.631620Z",
     "start_time": "2024-03-23T09:13:14.627465Z"
    }
   },
   "id": "cd68bec60e2d33c0",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:13:14.636571Z",
     "start_time": "2024-03-23T09:13:14.633144Z"
    }
   },
   "id": "ba66ac064b0e256d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# dependent on the implementation of the word2index and index2word\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "MAX_LENGTH = 80 + 2 # 80 words + SOS and EOS"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:33:11.689003Z",
     "start_time": "2024-03-23T09:33:11.677603Z"
    }
   },
   "id": "82cf8dea8a0ef552",
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:33:12.198217Z",
     "start_time": "2024-03-23T09:33:12.193116Z"
    }
   },
   "id": "9d67c1ab9f525cc0",
   "execution_count": 39
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def readLangs(to_spanish=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the cvs file\n",
    "    file = pd.read_csv(\"../data/cropped_en_es_translation.csv\")\n",
    "    en = file[\"en_translation\"]\n",
    "    es = file[\"es_translation\"]\n",
    "\n",
    "    # Split every line into pairs\n",
    "\n",
    "    # Reverse pairs, make Lang instances\n",
    "    if to_spanish:\n",
    "        pairs = list(zip(es, en))\n",
    "        input_lang = Lang(\"es\")\n",
    "        output_lang = Lang(\"en\")\n",
    "    else:\n",
    "        pairs = list(zip(en, es))\n",
    "        input_lang = Lang(\"en\")\n",
    "        output_lang = Lang(\"es\")\n",
    "        \n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0])\n",
    "        output_lang.addSentence(pair[1])\n",
    "        \n",
    "    return input_lang, output_lang, pairs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:33:12.278070Z",
     "start_time": "2024-03-23T09:33:12.271438Z"
    }
   },
   "id": "96ffffae0e32e967",
   "execution_count": 40
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n",
      "('In my opinion the European Union should have the right to be informed of the minimum levels of social benefits in the individual Member States ', 'Teniendo esto en mente el Consejo Europeo sugirió un límite del 1,5 para que el déficit de transposición de las directivas se aplicara a todos los Estados miembros ')\n"
     ]
    }
   ],
   "source": [
    "input_lang, output_lang, pairs = readLangs()\n",
    "for pair in pairs:\n",
    "    input_lang.addSentence(pair[0])\n",
    "    output_lang.addSentence(pair[1])\n",
    "print(random.choice(pairs))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.540992Z",
     "start_time": "2024-03-23T09:33:12.353174Z"
    }
   },
   "id": "54cc264a49496945",
   "execution_count": 41
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def indexesFromSentence(lang, sentence):\n",
    "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence):\n",
    "    indexes = indexesFromSentence(lang, sentence)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
    "    return input_tensor, target_tensor\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = readLangs()\n",
    "\n",
    "    n = len(pairs)\n",
    "    input_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "    target_ids = np.zeros((n, MAX_LENGTH), dtype=np.int32)\n",
    "\n",
    "    for idx, (inp, tgt) in enumerate(pairs):\n",
    "        inp_ids = indexesFromSentence(input_lang, inp)\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt)\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        input_ids[idx, :len(inp_ids)] = inp_ids\n",
    "        target_ids[idx, :len(tgt_ids)] = tgt_ids\n",
    "\n",
    "    train_data = TensorDataset(torch.LongTensor(input_ids).to(device),\n",
    "                               torch.LongTensor(target_ids).to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    return input_lang, output_lang, train_dataloader"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.549980Z",
     "start_time": "2024-03-23T09:34:40.542667Z"
    }
   },
   "id": "5db01f9739ea2e5f",
   "execution_count": 42
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output, hidden = self.gru(self.dropout(self.embedding(x)))\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:37:10.340718Z",
     "start_time": "2024-03-23T09:37:10.336792Z"
    }
   },
   "id": "46add9dcb9923925",
   "execution_count": 53
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden = self.forward_step(decoder_input, decoder_output)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            if target_tensor is not None:\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)\n",
    "            else:\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        return decoder_outputs, decoder_hidden, None\n",
    "\n",
    "    def forward_step(self, input, hidden):\n",
    "        embedded_rel = F.relu(self.embedding(input))\n",
    "        output, hidden = self.gru(embedded_rel, hidden)\n",
    "        output = self.out(output)\n",
    "        return output, hidden"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:37:10.548229Z",
     "start_time": "2024-03-23T09:37:10.541412Z"
    }
   },
   "id": "dba66bedbf53677c",
   "execution_count": 54
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.571103Z",
     "start_time": "2024-03-23T09:34:40.565782Z"
    }
   },
   "id": "c8d9447474cef911",
   "execution_count": 45
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class AttnDecoder(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.gru = nn.GRU(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden.permute(1, 0, 2)\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "        input_gru = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        output, hidden = self.gru(input_gru, hidden)\n",
    "        output = self.out(output)\n",
    "\n",
    "        return output, hidden, attn_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.580547Z",
     "start_time": "2024-03-23T09:34:40.572463Z"
    }
   },
   "id": "b74c9a2d2452fa33",
   "execution_count": 46
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train_epoch(dataloader, encoder, decoder, encoder_optimizer,\n",
    "                decoder_optimizer, criterion):\n",
    "    total_loss = 0\n",
    "    for data in tqdm(dataloader, leave=False):\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.586790Z",
     "start_time": "2024-03-23T09:34:40.582069Z"
    }
   },
   "id": "ba731c24fd7d868c",
   "execution_count": 47
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.593029Z",
     "start_time": "2024-03-23T09:34:40.588393Z"
    }
   },
   "id": "9cbbf855683a1b1",
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def train(train_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "          print_every=100, plot_every=100):\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.AdamW(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.AdamW(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    for epoch in tqdm(range(1, n_epochs + 1)):\n",
    "        loss = train_epoch(train_dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
    "        print_loss_total += loss\n",
    "        plot_loss_total += loss\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print(print_loss_avg)\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "    showPlot(plot_losses)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.599626Z",
     "start_time": "2024-03-23T09:34:40.594810Z"
    }
   },
   "id": "d2e2531c3024840d",
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.609658Z",
     "start_time": "2024-03-23T09:34:40.604769Z"
    }
   },
   "id": "d587f2b145c9606",
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:34:40.615810Z",
     "start_time": "2024-03-23T09:34:40.611670Z"
    }
   },
   "id": "ed9a5f95af5f0fbc",
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading lines...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]\n",
      "  0%|          | 0/60115 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/60115 [00:18<316:03:43, 18.93s/it]\u001B[A\n",
      "  0%|          | 2/60115 [00:29<232:11:54, 13.91s/it]\u001B[A\n",
      "  0%|          | 3/60115 [00:40<212:50:31, 12.75s/it]\u001B[A\n",
      "  0%|          | 4/60115 [00:51<201:40:08, 12.08s/it]\u001B[A\n",
      "  0%|          | 5/60115 [01:01<188:54:36, 11.31s/it]\u001B[A\n",
      "  0%|          | 6/60115 [01:12<184:59:33, 11.08s/it]\u001B[A\n",
      "  0%|          | 7/60115 [01:24<192:00:26, 11.50s/it]\u001B[A\n",
      "  0%|          | 8/60115 [01:37<196:43:58, 11.78s/it]\u001B[A\n",
      "  0%|          | 9/60115 [01:46<186:28:30, 11.17s/it]\u001B[A\n",
      "  0%|          | 10/60115 [01:58<198:38:44, 11.90s/it]\u001B[A\n",
      "  0%|          | 0/20 [01:58<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[57], line 9\u001B[0m\n\u001B[1;32m      6\u001B[0m encoder \u001B[38;5;241m=\u001B[39m Encoder(input_lang\u001B[38;5;241m.\u001B[39mn_words, hidden_size)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[1;32m      7\u001B[0m decoder \u001B[38;5;241m=\u001B[39m AttnDecoder(hidden_size, output_lang\u001B[38;5;241m.\u001B[39mn_words)\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m----> 9\u001B[0m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m20\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mprint_every\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1000\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mplot_every\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m5000\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[49], line 12\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(train_dataloader, encoder, decoder, n_epochs, learning_rate, print_every, plot_every)\u001B[0m\n\u001B[1;32m      9\u001B[0m criterion \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mNLLLoss()\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m tqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, n_epochs \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m)):\n\u001B[0;32m---> 12\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain_epoch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdecoder_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m     print_loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n\u001B[1;32m     14\u001B[0m     plot_loss_total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m loss\n",
      "Cell \u001B[0;32mIn[47], line 11\u001B[0m, in \u001B[0;36mtrain_epoch\u001B[0;34m(dataloader, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\u001B[0m\n\u001B[1;32m      8\u001B[0m decoder_optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[1;32m     10\u001B[0m encoder_outputs, encoder_hidden \u001B[38;5;241m=\u001B[39m encoder(input_tensor)\n\u001B[0;32m---> 11\u001B[0m decoder_outputs, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mdecoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mencoder_hidden\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget_tensor\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     13\u001B[0m loss \u001B[38;5;241m=\u001B[39m criterion(\n\u001B[1;32m     14\u001B[0m     decoder_outputs\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, decoder_outputs\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)),\n\u001B[1;32m     15\u001B[0m     target_tensor\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/NLP/NLP_Assigmnent2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/NLP/NLP_Assigmnent2/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[0;32mIn[46], line 33\u001B[0m, in \u001B[0;36mAttnDecoder.forward\u001B[0;34m(self, encoder_outputs, encoder_hidden, target_tensor)\u001B[0m\n\u001B[1;32m     30\u001B[0m         decoder_input \u001B[38;5;241m=\u001B[39m topi\u001B[38;5;241m.\u001B[39msqueeze(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\u001B[38;5;241m.\u001B[39mdetach()  \u001B[38;5;66;03m# detach from history as input\u001B[39;00m\n\u001B[1;32m     32\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(decoder_outputs, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m---> 33\u001B[0m decoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_softmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdecoder_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdim\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m     34\u001B[0m attentions \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mcat(attentions, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m     36\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m decoder_outputs, decoder_hidden, attentions\n",
      "File \u001B[0;32m~/Library/Mobile Documents/com~apple~CloudDocs/Documents/NLP/NLP_Assigmnent2/.venv/lib/python3.11/site-packages/torch/nn/functional.py:1947\u001B[0m, in \u001B[0;36mlog_softmax\u001B[0;34m(input, dim, _stacklevel, dtype)\u001B[0m\n\u001B[1;32m   1945\u001B[0m     dim \u001B[38;5;241m=\u001B[39m _get_softmax_dim(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlog_softmax\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim(), _stacklevel)\n\u001B[1;32m   1946\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m dtype \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1947\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43minput\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlog_softmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdim\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1948\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1949\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mlog_softmax(dim, dtype\u001B[38;5;241m=\u001B[39mdtype)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "hidden_size = 128\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "encoder = Encoder(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoder(hidden_size, output_lang.n_words).to(device)\n",
    "\n",
    "train(train_dataloader, encoder, decoder, 20, print_every=1000, plot_every=5000)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:43:05.991886Z",
     "start_time": "2024-03-23T09:39:40.575256Z"
    }
   },
   "id": "1a4d87614ce5d31e",
   "execution_count": 57
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-23T09:35:59.822189Z",
     "start_time": "2024-03-23T09:35:59.822093Z"
    }
   },
   "id": "d0f895b7a76af20b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4e30efc712dd3894"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "10d381ddb7a13c0c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
